\chapter{Reinforcement Learning Model}
\label{chap:RLModel}
\section{Introduction}
\label{sec:IntroRL}
In the last session we analyzed the task-related activity patterns of principal assembly-pair types.\\Significant task related activity was tested with Friedman test in stimulus presentation (CS+/-) interval, stimulus continued\footnote{The window right before the reward delivery time, or the expected reward time in False Alarm trials, or the hypothetical reward time in Correct Rejection trials.} (CS+/- cont.) interval, and reward\footnote{Or expected reward in False Alarm trials, or Hypothetical Reward in Correct Rejection trials.}.\\We found a good portion of SPN-DAN pairs becoming active early at stimulus presentation, another important group being active later during the stimulus presentation, and only a small fraction of SPN-DAN pairs active only at the retrieval.\\SPN-DAN activation broader than the FSN-DAN activation, and is worth to recall that the mean lag of FSN-DAN pairs is shorter than the mean lag of SPN-DAN pairs. Time scale and activation profile suggest that those two pair types reflect the two component of the prediction error coding. Specifically SPN-DAN pairs are good candidates for the signal identification and evaluation crucial for reward prediction and reward prediction error coding as defined in classical reinforcement learning model.\\On the other hand we observed that FSN-DAN pairs are early activated by the stimulus on onset and they phasic activate at the reward time. The early activation can corresponds the detection of the stimulus, namely the first component of reward prediction error signalling not directly involved in the learning, but crucial inasmuch as evolves in the identification and valuation component.\\FSN-DAN are homogeneous in their conditioned stimuli response, either rewarded or unrewarded, but extremely diversified in their activation in response to different unconditioned stimuli. In False Alarm trials a good portion of FSN-DAN is depressed before and after the expected reward time. Such activity could reflect emotional feelings of the animal during the task, that could be only partially involved in the reward prediction error definition given by reinforcement learning model.\\The analysis of task related responses cannot read the highly dynamic of task, the task was indeed such that the animal had to assign and re-assign a value to the rewarded odor to be able to predict the reward. Only taking in account this dynamic we can understand whether and how SPN-DAN and FSN-DAN pairs compute reward prediction error, and if different cell-assembly types specialize in different aspects of the complex task-related coding.\\To take all this into account, we used the reinforcement learning technique (\cite{RescorlaWagner}, \cite{Sutton}, \cite{SuttonBarto}).\\$"$Reinforcement learning$"$ is an area in machine learning, in which an agent tries to learn the best action to take, depending on the circumstances in which this action will be performed, this approach can incorporate any changes in the environment of the decision making process. In psychology, from where the idea of this machinery derives, the concept is known as “learning by reinforcement”: the agent receives a reward or punishment, depending on the decision made, through the experience he is able to associate the actions that generate the greatest reward for each situation that the environment presents, and to avoid unrewarded actions or actions that generate punishment.\\
Machine learning uses the same idea: the machine observes a state and, based on this, chooses an action to take and receives the reward associated with that specific action in that state, thus obtaining the information of this specific combination. The process is repeated until the machine is able to choose the best action to take for each of the possible scenarios to be observed in the future.\\In the contingencies of the experiment conducted in our lab, two odor were presented and only one was associated with a reward with a probability of 0.9. Each time one odor was presented the mouse chose to take the action (lick or not lick) that maximized the reward and minimized his effort. the task was learnt when the animal was able to lick for the rewarded odor and not lick when the unrewarded odor was presented.\\To model this learning process we set up a Rescorla-Wagner model with Pearce Hall update mechanism (\cite{RescorlaWagner}, \cite{PearceHall}, \cite{Li}, \cite{Costa}, \cite{Koppe}).\\

\section{Model}
\label{sec:Model}
%%%%%{\color{blue}Start writing equation of reinforcement learning}
%%%%%First Option I tried, Georgia's option {\color{red}cite Georgia's paper}
%%%%%We set up four RL models and we chose the one that better fitted the behaviour for the regression analysis.
Reinforcement learning model are usually set up as follows:
\begin{itemize}
    \item At each trial $t$ of the experiment, a state $s$ is observed, this is an input of the model.
    \item After the stimulus presentation, the animal has the chance to take an action, and according to choice made, the reward is delivered. The reward is a vector $r(t)$, each vector component indicates the reward value delivered at the trial $t$, the reward vector is an input of the model.
    \item After the reward delivery, the animal computes the reward prediction error, that is evaluated in the model as the difference between the actual reward and the expected reward.
    \item States values are the expected reward values and they are updated according to the last expected values and the reward prediction error and it is modulated by the learning rate.
    \item The learning rate is often a constant free parameter, however it has been shown that a time dependent variable better describes the learning dynamic (\cite{Funamizu}, \cite{Daw}).
    \item Learned values are then translated into action probabilities via a softmax function, maximized with respect some free parameters.
\end{itemize}

We set up a Q Learning-Forgetting model (Q L-F model) starting from an hybrid Rescorla-Wagner/Pearce-Hall update mechanism (\cite{Koppe}, \cite{Costa}, \cite{Li}), our model is also provided of an update of unchosen option using learning and forgetting parameters (\cite{ItoDoya1}, \cite{Katahira}).
The model assigns each state $s$ and action $a$, an action value $Q_{s,a}(t)$ where $t$ is the index of the trial. In our specific case we have two states, corresponding to the two odors presented and two possible actions, i.e., to lick or not to lick. 
Based on the set of action values the model transforms the The action value for the chosen option is updated by
\begin{equation}
Q_{s,a}(t+1)  = Q_{s,a}(t)+k_L\cdot\alpha_L(t)\cdot\delta(t), \hspace{0.3cm} with \hspace{0.3cm}\delta(t)=r(t)-Q_{s,a}(t-1)
\label{eq:Qlearning}
\end{equation}
where $\delta$ is the prediction error, i.e., the difference between the reward $r(t)$ and the expected reward value $Q_{s,a}(t)$ for an action $a$ given a state $s$, $k_L$ is the learning parameter and $\alpha_L(t)$ is the Pearce-Hall associability for the chosen option, is a trial-dependent rate component which adjusts in accordance with the average accuracy of recent predictions, evolving by
\begin{equation}
   \alpha_L(t)=(1-\eta)\cdot\alpha_L(t-1)+\eta\cdot\abs{\delta(t)},\hspace{0.3cm} \eta\in[0,1]
    \label{eq:Alphalearning}
\end{equation}
Note that the $n$'s associability depends on absolute prediction errors from past trials, but not the current one ensuring that $\delta(t)$ was not double counted in the value update.\\The associability term in eq. \ref{eq:Alphalearning} measures the attention that the animal has to put to the cue, in other terms it is nothing but the uncertainty of the animal to get the reward. 
For the unchosen option $a'\neq a$ the action value is updated by
\begin{equation}
    Q_{s,a'}(t+1) = Q_{s,a'}(t)-k_F\cdot\alpha_F(t)\cdot Q_{s,a'}(t)
    \label{eq:Qforgetting}
\end{equation}
where $k_F$ is the forgetting rate (\cite{ItoDoya1}), in our model the associability for the unchosen option is as well a time-dependent component and evolves as follows
\begin{equation}
    \alpha_F(t)=(1-\eta)\cdot\alpha_F(t-1)+\eta\cdot Q_{s,a'}(t-1), \hspace{0.3cm}
    \eta\in[0,1]
    \label{eq:Alphaforgetting}
\end{equation}
Learned values are then translated into action probabilities via a softmax function:
\begin{equation}
p(a|s)=\frac{e^{\beta Q_{s,a}(t)}}{\sum_l e^{\beta Q_{s,l}(t)}}
\label{eq:ProbMod}
\end{equation}
where $\beta$ is a free parameter, usually called inverse temperature, that governs the animals$'$ exploitation/exploration.\\
Note that using $k_F = 0$ the model is reduced to the hybrid Rescorla Wagner model. We applied to our data four different version of the model, the one shown here as presentation is the one that better fit the behavioural data. The other three versions are:
\begin{description}
    \item[i.] \textbf{Rescorla-Wagner.} The presented model is reducible to an hybrid Rescorla Wagner/Pearce Hall update mechanism when we use a learning parameter $k$, and no forgetting parameter, i.e. $k_F = 0$.
    Then, if $a$ represents the chosen option and $a'\neq a$ the unchosen option, the action values evolve as follows\\
    $\begin{array}{lcl}
    Q_{s,a}(t+1)&=& Q_{s,a}(t)+k\cdot\alpha_L(t)\cdot\delta(t), \hspace{0.3cm} with \hspace{0.3cm}\delta(t)=r(t)-q_{s,a}(t)\\
    Q_{s,a'}(t+1)&=&Q_{s,a'}(t)\\
    \end{array}$
    \item[ii.] \textbf{Rescorla-Wagner with two learning rate modulations.} Let $a(t) \in \{1,0\}$ denote the option was chosen at trial $t$, where $1$ stands for "to lick" and $0$ stands for "not to lick". We introduce then two learning parameters $k_{1}$ and $k_{0}$ respectively for the choice $a=1$ and $a=0$ to be estimated by the model, while $k_F$ is fixed to zero.
    Then when the choice $a=1$ is realized, the action values evolves by\\
   $\begin{array}{lcl}
       Q_{s,1}(t+1)&=&Q_{s,1}(t)+k_1\cdot\alpha(t)\cdot\delta(t)\\
         Q_{s,0}(t+1)&=&Q_{s,0}(t)\\ 
    \end{array}$\\
    while if the choice $a=0$ is realized we have\\
    $\begin{array}{lcl}
       Q_{s,0}(t+1)&=&Q_{s,0}(t)+k_0\cdot\alpha(t)\cdot\delta(t)\\
         Q_{s,1}(t+1)&=&Q_{s,1}(t)\\ 
    \end{array}$
    \item[iii.] \textbf{Rescorla-Wagner with two $\eta$ parameters.} We use a learning model, without forgetting parameters, and we introduce two $\eta$ parameters $\eta_1$ and $\eta_0$ for the choice $1$ and $0$ respectively, to be estimated by model. The associability $\alpha_L$ evolves as follow:\\
   $\begin{array}{lcll}
    \alpha_{L}(t) & = & (1-\eta_1)\cdot\alpha_{L}(t-1)+\eta_1\cdot\abs{\delta(t)} & \mbox{if\,\,}  a=1,\\
    \alpha_{L}(t) & = & (1-\eta_0)\cdot\alpha_{L}(t-1)+\eta_0\cdot\abs{\delta(t)} & \mbox{if\,\,}  a=0.\\
    \end{array}$\\
    and action values for the chosen option evolving consequently as in the equation \ref{eq:Qlearning}.
\end{description}
In figure one example of animal performance and RL-models action
values. When the odor is rewarded the expectancy of reward related to the lick
action reproduce the hit performance as one could intuitively predict, while, for non
rewarded odor, the reward expectation associated to the no-lick action decrease as
the correct rejection performance increase. Action values associated to lick for non
rewarded odor and non-lick for rewarded odor give us an estimation of false alarm
and miss trials, namely respectively trials in which the odor was not rewarded and
the mouse went for the reward, or in which the odor was rewarded and the mouse
sat quiet. That happened especially as the experiment or the new phase began for
the first 5-10 trials of the phase.
\begin{figure}
    \centering
    \includegraphics[scale=0.42]{figures/PerfEndrevAn1.png}
    \includegraphics[scale=0.42]{figures/QValuesEndrevAn1.png}
    \includegraphics[scale=0.42]{figures/AlphaEndrevAn1.png}
    \includegraphics[scale=0.42]{figures/DeltaEndrevAn1.png}
    \caption{In figure one example of animal performance and RL model action
values. Odor 1 was rewarded only in the original phase, while Odor 2 was rewarded
only in the reversal phase. In order from the top to bottom: Performance in all trials, in hit trials and correct rejection trials. RL model action values for the action lick, when odor 1/odor 2 occurs (blue/red) and the action no lick when odor 1/odor 2 is presented
(green/black). Uncertainty $\alpha_L(t)$ for the chose option (red line), and $\alpha_F(t)$ for the unchosen option (blue line). Prediction error $\delta(t)$.}
    \label{fig:L-Fmodel}
\end{figure}
\section{Model Estimation and Selection}
\label{sec:Behavior}
Given the action probabilities as defined by equation \ref{eq:ProbMod},the model log-likelihood can be expressed as $l=\sum_{t} \log p(a_t|s_t,Q_t,\theta_t)$. Where $\theta$ are the model parameters, as defined model by model in the previous session, which were estimated for each animal by maximum likelihood using MATLAB$'$s active set algorithm (fmincon), which relies onsolving the Karush-Kuhn-Tucker equations by quasi-Newton methods, starting from 100 different initial conditions to avoid local minima. The maximized log-likelihood value was later entered into a model comparison analysis\footnote{Part of the scripts used were adapted from some previous scripts developed and provided by dr. Georgia Koppe}.\\
The hybrid Rescorla-Wagner model (i.) has fewer parameters with respect to the three adapted Rescorla-Wagner versions proposed (ii.,iii., L-F model), and all the three versions can be transformed to the hybrid Rescorla-Wagner by imposing constraints on the former's parameters. Thus using Likelihood Ratio tests for nested models we could test for each alternative model proposed whether it was better than the hybrid Rescorla-Wagner. Likelihood ratio test (LRT) is formalized as the ratio of the likelihood functions of the models to compare (\cite{NeymanPearson}, \cite{King}) as follows:\\
 \hspace{5cm} $LRT = -2 \log (\frac{\mathcal{L}_s(\hat{\theta})}{\mathcal{L}_g(\hat{\theta})})$\\
%\begin{equation}
%LRT = -2 \log (\frac{\mathcal{L}_s(\hat{\theta})}{\mathcal{L}_g(\hat{\theta})})
%\label{eq:LRT}
%\end{equation}
where $\hat{\theta}$ are the parameters values that maximize the likelihood function.
The simpler model (s) has fewer parameters than the general model (g), the latter can be reduced to the simpler model after imposing some constraint (null hypothesis).\\
LRT compares the fit of two models and has an asymptotic $\chi^2$ distribution. The null hypothesis is that the smaller model is the $"$best$"$ model and it is rejected when the test statistic is large. In other words, if the null hypothesis is rejected, then the larger model is a significant improvement over the smaller one.\\
In this way, in each test we tested whether the addition of one parameter to the pure Rescorla-Wagner model was needed and brought an improvement. The tests were conducted separately for each animal.\\In the comparison between the hybrid Rescorla-Wagner (hRW) and the learning-forgetting model (QLF), the second model reveals to be the $"$best$"$ model in $95\%$ of cases.\\
In table \ref{tab:ModelComparison} LRT and the relatives $\chi^2$ statistic and p-values in two paradigms.
\begin{table}[H]
\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
\multicolumn{4}{|c|}{\cellcolor{blue!25}{Comparison between hRW model and QLF model (Likelihood ratio test)}}\\
\hline
 \cellcolor[gray]{0.9} animal & \cellcolor[gray]{0.9} $\mathcal{L}_s / \mathcal{L}_g$& \cellcolor[gray]{0.9} $\chi^2 statistic$ & \cellcolor[gray]{0.9} p-value\\
 \hline
 1& 1.77e-25 & 113.98 & \colorbox{cyan}{0}\\
 \hline
 2& 4.02e-08 & 34.05 & \colorbox{cyan}{5.35e-09}\\
 \hline
 3 & 1.34e-04 & 17.82 & \colorbox{cyan}{2.42e-05}\\
 \hline
 4 & 3.43e-34 & 154.10 & \colorbox{cyan}{0}\\
 \hline
 5 & 6.18e-19 & 83.85 & \colorbox{cyan}{0}\\
 \hline
 6 & 1.22e-08 & 36.44 & \colorbox{cyan}{1.57e-09}\\
 \hline
 7 & 1.62e-24 & 109.55 & \colorbox{cyan}{0}\\
 \hline
 8 & 3.77e-10 & 43.39 & \colorbox{cyan}{4.47e-11}\\
 \hline
 9 & 0.95 & 0.09 & \colorbox{yellow}{0.75}\\
 \hline
 10 & 2.87e-05 & 20.91 & \colorbox{cyan}{4.80e-06}\\
 \hline
 11 & 1.25e-38 & 174.53 & \colorbox{cyan}{0}\\
 \hline
 12 & 2.37e-05 & 21.30 & \colorbox{cyan}{3.93e-06}\\
 \hline
 13 & 8.16e-07 & 28.03 & \colorbox{cyan}{1.19e-07}\\
 \hline
 14 & 3.99e-02 & 6.44 & \colorbox{cyan}{1.11e-02}\\
 \hline
 15 & 7.37e-16 & 69.68 & \colorbox{cyan}{1.11e-16}\\
 \hline
 16 & 7.24e-43 & 194.06 & \colorbox{cyan}{0}\\
 \hline
 17 & 2.67e-05 & 21.06 & \colorbox{cyan}{4.45e-06}\\
 \hline
\end{tabular}
\caption{Likelihood ratio test performed in 17 animals. The null hypothesis is that the model with fewer parameters is the best model, this hypothesis is tested through the $\chi^2$ statistic. If the $\chi^2$ statistic is large the null hypothesis is rejected. In last column the p-value of the $\chi^2$ statistic, in blue p-values below significance level $\alpha=0.05$ indicate that the null hypothesis is rejected. In yellow not significant p-values.}
\label{tab:ModelComparison}
\end{table}
A further test was needed to understand whether the Learning-Forgetting was the best model among the three adapted version of the hybrid Rescorla-Wagner. In this case, since the models are not nested and they have same number of parameters, the best model was evaluated using the Bayesian Information Criterion (BIC), that is is a criterion for model selection among a finite set of models where the model with the lowest BIC is preferred (\cite{Schwarz}, \cite{NeathCavanaugh}). BIC is formally defined as 
\begin{equation*}
    BIC=k\log(n)-2\log(\hat{\mathcal{L}})
\end{equation*}
where:
${\hat{\mathcal{L}}}$ is the maximized value of the likelihood function of the model, $n$ is the sample size, $k$ is the number of parameters estimated by the model.
Following Raftery’s approach (\cite{Raftery}), a difference of BIC lower than 2 between two models is barely worth mentioning, a difference between 2 and 5 is positive, a difference between 6 and 10 is strong, and a difference larger than 10 is very strong .\\
The Bayesian Information Criterion (BIC) the L-F model resulted to be the best model figure \ref{fig:BIC}.\\
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/BIC_Value.png}
    
    \vspace{1cm}
    
    \includegraphics[scale=0.5]{figures/DeltaBIC.png}
    \caption{On top BIC Information Criterion for the three models proposed as alternative of the hybrid Rescorla-Wagner. The BIC was computed separately for each animal, and we report the mean and the standard error. The best model is the model with the lower BIC, indicated as BIC*. The L-F model revealed to be the best model for the more than 80$\%$ of the animals. The $2\eta$ model was never the best model and the $2\kappa$ model was the best model for less the 20$\%$ of the animals, it is worth to notice that also when the L-F model was not the best model, the difference between its BIC and the BIC* rarely passed the value 5. On bottom the difference between the BIC of each model (BIC$_{model}$) and BIC*, this difference was computed separately for each animal and we report the mean and the standard error.}
    \label{fig:BIC}
\end{figure}
In figure \ref{fig:4models} action values of the four proposed models. In the gray box the animal's performance is reported as indication of actions taken in task. The importance of the forgetting term is particularly clear if we look at the action values $Q_{1,0}(T)$ in original phase and $Q_{2,0}(t)$ in reversal phase curves. The unchosen option (which often corresponds to  not-licking for rewarded odor when the task is learnt) does not update the action values in models that do not provide the forgetting term. Which implies that the action value of rewarded odor and not-licking action remains stable at the value of the last update when the non-licking action was chosen (see $Q_{1,0}$ in A.,B.,C.), that however does not correspond to the real value assigned by animal persistently licking when that odor is presented. The learning-forgetting model instead reproduces in this sense the entire perspective of learning dynamic, assuming that the unchosen option is consciously not taken at the moment that the opposite action is chosen. The small increases of $Q_{1,0}$ and $Q_{2,0}$ that occur respectively before trial no.50 and after trial no.150 is due to the fact that when the animal is too reluctant to lick a reward is given to encourage the lick choice for the rewarded odor. 
\begin{figure}
    \centering
    \includegraphics[scale=0.75]{figures/Resume4models3.png}
    \caption{Animal performance (top left) compared to the action values of the four models (A.B.C.D.)}
    \label{fig:4models}
\end{figure}

%To see whether the neural activity is correlated with the behaviour, we regress the neuronal activity and the assembly activity with the Q L-F model action values $Qs$, error prediction $\delta$, and associability $\alpha$ and behavioural measures. 
\section{Uncertainty and prediction error signal in pair-types}
\label{sec:CorrRL}
We assume VS-VTA interactions being able to predict the reward and compute prediction error. We used a reinforcement learning model to parameterize the learning functions, if the assembly-pairs code for some aspects of the learning they correlate with some reinforcement learning functions.\\
In \hyperref[sec:TaskResp]{~Section \ref*{sec:TaskResp}} we hinted which kind of trial variability in assembly activity we expect when the assembly predicts the reward and computes prediction error, doing a parallelism with the dopamine neurons response in case of probabilistic reward \cite{Fiorillo}.\\
If the assembly-pairs activity codes for the reward prediction we expect intense activity in the CS window during the stable phase, when the animal is able to predict the reward, this case in the probabilistic reward comparison is similar to the case in which the probability to get the reward is equal to 1 or high (see figure \ref{fig:RewPred} top); whereas during the unstable phase we expect few activation in CS window because the animal is uncertain to get the reward, this case sis comparable with the case of reward probability equal to 0.25 or below (see figure\ref{fig:RewPred} bottom). In other words we expect the assembly-pair activity in CS window to anti-correlate with the uncertainty $\alpha_L(t)$, indeed the uncertainty is high at the beginning of the task and goes down at the end of the original phase, to raise up again at the beginning of the reversal phase. 
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{figures/PreRegress.png}
    \caption{(A) Comparison between the uncertainty $\alpha(t)$ and the SPN-DAN assembly pairs activity in the stable and unstable phase. For this example only trials in which the rewarded odor was presented are shown. The analysis was restricted to the CS window, because the prediction of the reward happens immediately after the stimulus onset. If assembly pairs code for reward prediction, in the unstable phase characterized by high uncertainty to get the reward the assembly-pairs activity in CS window is low, whereas in the stable phase characterized by low uncertainty the assembly-pairs activity is high. The difference between the assembly-pairs activity in stable and unstable phase is significantly different to zero (B).}
    \label{fig:StableUnstableAlphaCS}
\end{figure}In figure\ref{fig:StableUnstableAlphaCS} we show an intuitive way to use the task variability in order to see the correlations with model functions. We divided the task in two phases: a first phase constituted by first trials in which we think the animal is learning the task and is uncertain about the reward outcome, we call this phase the unstable phase; a second phase is on the other hand constituted by last trials of the task in which the animal has learnt the task rule and it is more certain about the reward outcome when a rewarded or unrewarded odor is presented.\\We compare the assembly-pairs activity in unstable and stable phase. The difference between the two phases is significantly different from zero only for SPN-DAN pairs.\\This found highlights a possible specificity of assembly-pairs types coding, that will be proved in the next sessions.\\
With similar arguments we can assume that, if the assembly-pairs compute reward prediction error their activity should correlate with the prediction error $\delta(t)$ in the US window. At the beginning of the task, when the prediction error is high we expect in US window high assembly-pairs response in conformity with the high surprise of the animal to get the reward, while when the task is learnt the surprise to get the reward and the prediction error are smaller and we expect less activity.\\
In the next session we mathematically formalize these concepts in order to quantify the correlations with $\alpha(t)$ and $\delta(t)$ an thus prove whether there is a specific VS-VTA interaction path involved in the reward prediction coding and prediction error computing. 


\section{Regression}
\label{sec:Regression}
In the previous sessions we presented the Q learning-forgetting model to parameterize the learning functions. Our main questions are: do assembly-pairs predict the reward and compute prediction error? and do different assembly-pair types compute different component of prediction error? and more specifically which assembly-pair types compute the evaluation component of the prediction error parameterized in reinforcement learning model? As discussed in the previous session, if an assembly-pair computes reward prediction error we expect the assembly-activity being anticorrelated with the uncertainty $\alpha(t)$ in the CS window and correlated with the prediction error $\delta(t)$ in the US (real or expected reward) window. In order to quantify these correlations we regress the assembly-pair activity with $\alpha (t)$ and $\delta (t)$. The two regressions are built separately: we regress out the uncertainty $\alpha(t)$ in the CS window and the prediction error $\delta(t)$ in the US (real or expected) window.\\
Since the majority of the inter-regional pairs follow a Poisson distribution (see figure \ref{fig:DistributionEx}), we selected these pairs and built two generalized linear models with logarithmic link function, tailored for Poisson distributed data.
The assembly-pairs activity is a sample of n observations $\mathbf{Y}=(y_1, y_2,..., y_n)$ which can be treated as realizations of independent Poisson random variables, with $Y\sim P(\mu)$, in generalized linear model the mean (and therefore the variance), $\mu$, of the distribution depends on the independent variables, $X$, through:
$E (\mathbf {Y} )={\boldsymbol {\mu }}=g^{-1}(\mathbf {X} {\boldsymbol {\beta }})$ where $g$ is the link function, logarithmic in our case, $\boldsymbol{\beta}$ are unknown parameters estimated by the model, through which is quantified the relationship between the dependent and independent variables. The two regression models used can be written as follow:
\begin{equation}
    \log(\mu_t)=\beta_0+\beta\cdot\alpha(t-1)
    \label{eq:regrAlpha}
\end{equation}
we notice that in equation \ref{eq:Qlearning} we use the uncertainty of the trial $t$ ($\alpha(t)$) to update the reward expectation value of the trial $t+1$ ($Q_{s,a}(t+1)$), the uncertainty $\alpha(t)$ is updated only after the update of $\delta(t)$, which happens at the reward time, then in the CS window of the trial $t$, where the update is not yet computed, we regress out the uncertainty $\alpha(t-1)$ with the assembly-pair activity of the trial $t$.
\begin{equation}
    \log(\mu_t)=\beta_0+\beta\cdot\delta(t)
    \label{eq:regrDelta}
\end{equation}
On the other hand we regress out the prediction error $\delta$ in US the window, which start either at the reward time or at the expected reward time when the unrewarded odor is presented (see \hyperref[sec:TaskResp]{~Chapter \ref*{sec:TaskResp}} for the window specification). In this window the update is already computed, thus we regress out the prediction error of the trial $t$ with the assembly activity at the trial $t$.\\
Before to present the results a clarification about the meaning of the coefficient $\beta$ in Poisson regression is needed.\\
Coefficients $\beta$ tell us how changes in the independent variables are associated with changes in the dependent variable.
The two regression presented can be written as Poisson regression with one independent variable $x$:
\begin{equation}
    \log(\mu)=\beta_0+\beta_1\cdot x
    \label{eq:betaCoeff}
\end{equation}
So, if we have an initial value of the covariate $x_0$, then the predicted value of the mean $\mu_0$ is given by:
\begin{equation*}
    \log(\mu_0)=\beta_0+\beta_1\cdot x_0
    \label{eq:betaCoeff0}
\end{equation*}
If we now increase the covariate by 1, we get a new mean $\mu_1$,
\begin{equation*}
    \log(\mu_1)=\beta_0+\beta(x_0+1)=\beta_0+\beta_1x_0+\beta_1=\log(\mu_0)+\beta_1
    \label{eq:betaCoeff1}
\end{equation*}
So the log of the mean of $Y$ increases by $\beta_1$ when we increase $x$ by 1.
In general we can say that in Poisson regression, the regression coefficients $\beta$ are interpreted as the difference between the log of expected counts, where formally, this can be written as:
\begin{equation}
    \beta=\log(\mu_{t+1})-\log(\mu_t)
    \label{eq:BetaRelLog}
\end{equation}
But we are not really interested in how the log mean changes, we would like to know on average how $Y$ changes. 
Let do the exponential transformation:
\begin{equation}
    \mu_{t+1}=\mu_t\cdot \exp(\beta)
    \label{eq:BetaRelExp}
\end{equation}
It is important to note the difference with the linear regression: in normal linear regression the relationship between the observations and $\beta$ is additive, whereas in Poisson linear regression the relationship is multiplicative.\\
The equation \ref{eq:BetaRelExp} can be written in term of the percentage increase of the count number after dividing both members of the equation by $\mu_t$ and subtracting both members by 1, doing so we obtain:
\begin{equation}
\exp(\beta)-1=\frac{\mu_{t+1}-\mu_t}{\mu_t}
    \label{eq:BetaPerc}
\end{equation}
The last equation can be interpreted as the percentage increase of the count number.\\
This interpretation allow us to use the coefficients  $\beta$ in order to understand how changes in percentage the assembly activity in relation to the change of the variables $\alpha$ and $\delta$.\\Since we compare results among different assembly-pairs and sessions we need to standardize the coefficients $\beta$ resulting from the regression. Standardized $\beta$ can be expressed through the following:
\begin{equation}
    \beta^*=\beta\cdot\frac{\sigma(x)}{\sigma(Y)}
    \label{eq:betaStand}
\end{equation}
Results of two regressions are reported in figure \ref{fig:AlphaDeltaReg}.\\
To easily read the plot in the term of percentage instead of the $\beta$ coefficients we report the $\beta^*$ transformed as:
\begin{equation}
    \beta^*\rightarrow \exp(\beta^*)-1
    \label{eq:BetaPlot}
\end{equation}
We report only the statistically significant coefficient. After the pruning of only Poisson distributed assembly, and considering only statistically significant associations between the response and the regressor, we have in total the $33\%$ of SPN-DAN assembly-pairs when we regress out $\alpha$ and the $25\%$ of SPN-DAN assembly-pairs regressing out $\delta$.\\In figure \ref{fig:AlphaDeltaReg} the $\beta^*$ coefficient as defined above resulting from regression using the model with $\alpha$ as regressor defined in equation \ref{eq:regrAlpha} (figure \ref{fig:AlphaDeltaReg} top left) and from regression using the model with $\delta$ as regressor defined in equation \ref{eq:regrDelta} (figure \ref{fig:AlphaDeltaReg} bottom left). $\beta^*$ above indicates a positive correlation between the assembly-pair activity and the model variable $\alpha$ ($\delta$) whereas a $\beta^*$ below zero indicates a negative correlation. The value of $\beta^*$ indicates the change in percentage in assembly activity in relation to a change of one unit of the variable $\alpha$ ($\delta$). On the right side of the plots the empirical cumulative distribution function (ECDF) of $\beta^*$ related to the regression with $\alpha$ (top right) and with $\delta$ (bottom right). In purple SPN-DAN assembly-pairs, in dark green FSN-DAN assembly-pairs. The sign of correlation between assembly-pairs and regressors is extremely clear when we look at ECDF of $\beta^*$: for SPN-DAN pairs in case of regression with $\alpha$, ECDF is dramatically asymmetric in favour of negative $\beta^*$, whereas it symmetrical ditributed for FSN-DAN pairs. On the other hand the asymmetry of ECDF for SPN-DAN pairs is inverted when we regress out $\delta$, whereas it remain approximately symmetric for FSN-DAN pairs.\\We conclude that SPN-DAN assembly-pairs anticorrelate with the uncertainty $\alpha$ in CS window and correlate with prediction error $\delta$ in US window. On the other hand FSN-DAN do not show any preferential direction in correlation with $\alpha$ and $\delta$ and the $\beta^*$ are equally distributed around zero. These results confirm the hypothesis that SPN-DAN pairs specifically convey the reward prediction error signal.\\
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.25]{figures/Rev2An2As1_rew.png}
    \hspace{1cm}
    \includegraphics[scale=0.25]{figures/Rev2An2As2.png}
    
    \vspace{1cm}
    
    \includegraphics[scale=0.25]{figures/Rev2An2As1.png}
    \hspace{1cm}
    \includegraphics[scale=0.25]{figures/Firstrev1An4As5.png}
    \caption{Some example of pair activity distributions in CS and US window. Assembly-pair distribution can be approximated with Poisson's distribution. To regress out the assembly-pair activity with $\alpha(t)$ and $\delta(t)$ we use a Poisson linear model which is tailored for Poisson distributed observations.}
    \label{fig:DistributionEx}
\end{figure}\\
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.45]{figures/alphaRegrNew3.png}
    
    \vspace{1cm}
    \includegraphics[scale=0.45]{figures/deltaRegr3.png}
    \caption{Top: Results obtained regressing out $\alpha$. \textbf{Top left:} Box plot of standardized $\beta^*$ regression coefficients. In Poisson linear model the coefficient $\beta$ indicate the variation in percentage of the observations values when the dependent variable increase of one unit. \textbf{Top right:} empirical cumulative distribution function of $\beta^*$}
    \label{fig:AlphaDeltaReg}
\end{figure}\\
\section{Conclusion}
\label{sec:concRL}
After built a clear hypothesis on the specific role of different interregional assembly-pair types in reward prediction error coding (see \hyperref[sec:TaskResp]{~Chapter \ref*{sec:TaskResp}}), we needed to parameterize the learning functions through a reinforcement learning model, in order to establish a mathematical relationship between the assembly-pair activity and the functions mimicking the reward prediction signal.\\The reward prediction error is defined in reinforcement learning model theory as the difference from the expected reward value, which evolves in the model according to a defined learning dynamic, and the received outcome. As discussed in \hyperref[sec:TaskResp]{~Chapter \ref*{sec:TaskResp}} this component of prediction error signal is the valuation component, involved in the effective valuation of the signal, that follows an initial identification of the stimulus.\\We set up four model and we assessed the best model with Likelihood ratio test, when the models to compare were nested, and the Bayesian Information Criterion when the models to compare had same number of parameters.\\The winner model resulted to be a learning-forgetting model with dynamical learning rate, that mimics the uncertainty of the animal to get the reward. This variable $\alpha$ is equally important as the reward prediction error $\delta$ in prediction error coding.\\Our hypothesis, based on task related response of assembly-pair types and on the length of average inter-units lag distribution, was that SPN-DAN pairs are involved in second component of prediction error signal, parameterized by the model. On the other side FSN-DAN pairs could be involved only in the first component of prediction error signal because of their early and diversified activation, which let guess that those pairs unspecifically respond to stimulus or their action is involved in non-trivial emotional coding. We do not expect the FSN-DAN pairs response being represented by the learning functions.\\This hypothesis bears the idea of an highly specificity in dopamine circuit.\\To mathematically formalize this concept, we expected that SPN-DAN pairs anticorrelate with the uncertainty $\alpha$ in the stimulus window, being $\alpha$ the variable mimicking the uncertainty, and being the prediction error signal more strong at the stimulus presentation as the certainty to get the reward increases. On the other hand, if it is true that the prediction error signal is formed in SPN-DAN interactions, we expect as well SPN-DAN pairs to correlate with the prediction error $\delta$ at the retrieval.\\We modeled to linear Poisson regression model to regress out the uncertainty $\alpha$ and the prediction error $\delta$ with the assembly-pair activity. In figure \ref{fig:AlphaDeltaReg} we show on the left side the box-plots of resulting regression parameters when we regress out $\alpha$ (top left) and $\delta$ (bottom left). Since $\beta^*$ are approximately normally distributed, we plotted the $\beta^*$ in such a way that points in box plot are layed over a $1.96$ SEM ($95\%$ confidence interval) in the coloured box. Row data are jittered along x-axis for clarity. On the right side of figure \ref{fig:AlphaDeltaReg} the empirical distribution function corresponding to $\beta^*$ shown on the left.\\
The resulting regression coefficients show, as predicted, that SPN-DAN pairs negatively correlate with the uncertainty and positively correlate with the prediction error, on the other side FSN-DAN pairs do not show any significant correlation either with the uncertainty or the prediction error.\\We proved that the evaluation component of prediction error signal is formed by the SPN-DAN interaction. 
