\chapter{Conclusion and discussion}
\label{chap:Conclusion}
Neuronal models of reinforcement learning assume interactions of midbrain dopaminergic neurons in ventral tegmental area (VTA) and ventral striatum (VS) to compute the differences between anticipated and received outcomes, i.e. they compute the reward prediction error (RPE).\\We proposed here a study of the formation of RPE signals in VS-VTA interregional assemblies during learning to broaden our knowledge of the underpinnings of such signals. From this study we have drawn the following conclusion:
\begin{itemize}
    \item \textbf{Assembly-pairs occurrence, time scales and directionality:}\\We recorded neuronal activity in VS, including pallidum (VP), and VTA during a reversal learning go-no task in mice. In VS striatal projection neurons (SPN) and fast spiking neurons (FSN) were the more frequent cells types. In VTA we recorded a good fraction of gabaergic (GABA) and dopamine neurons (DAN). In this data set we further applied an unsupervised cell-assemblies detection method, to detect assemblies of synchronously ($lag=0$) and sequentially ($lag\neq0$) active units at arbitrary time scales ($\Delta$).\\The $\chi^2$ test revealed that SPN and DAN had high tendency to agglomerate in assembly together, as well as FSN and GABA.\\While we observed assemblies of temporal precision at the scale of few tens of milliseconds only within either VS or VTA, assemblies of lower temporal precision were detected across VS-VTA units. The temporal precision of this last group displayed a bimodal distribution with peaks around hundred milliseconds and one second.\\We focused on the more precise temporal scale first because these times did not interfere with the characteristic temporal scales of the experiment, such as the odor duration; second because the reward prediction error signals typically involve fast temporal scales (few hundred milliseconds). Interestingly the lags of more temporally precise assemblies displayed an asymmetric distribution indicating VS leading VTA.\\ Specifically, these directional assemblies were composed of SPN and DAN neurons. This was a functional directionality, the units in assemblies did not have to be directly connected by a synapse. Indeed, the SPN-DAN communication could occur both directly and relayed through interspersed neurons.\\In summary, we revealed here the multiple time scale and functional network of the striatum mid-brain interaction during a reversal assignment task.  
    \item \textbf{Assembly-pairs activity patterns:}\\
    Assemblies with different time scales and directionalities segregated different task related activity patterns.\\A large fraction of SPN-DAN assembly-pairs became selectively active at the rewarded stimulus onset, whereas a small fraction were active at the time of the reward retrieval. This kind of activation resembled the stereotypical RPE signals. Indeed RPE signal is formed after few hundred milliseconds from the stimulus onset (\cite{Tobler2003}, \cite{Nomoto2010}, \cite{Schultz2016}).\\FSN-DAN pairs were unselectively activated by rewarded or unrewarded stimuli. Furthermore their activation was diverse among different assembly-pairs, thereby suggesting that those assembly-pair types could be involved in motivational or hedonic signals rather than in RPE signals.\\These results showed that different assembly-pair types with DAN had different reward-related coding features. We putted forth the concept that SPN-DAN specialized in the valuation of component RPE. This component constitutes biological implementations of the crucial error term for reinforcement learning according to the Rescorlaâ€“Wagner-like models and temporal difference reinforcement models. 
    \item \textbf{Correlation with reinforcement learning model function:}\\The activity patterns gave us information about the average activity across trials in different moments of the trial, considering how the signal evolves within the trial, but without considering the trial by trial evolution. Nevertheless, the assembly-pair activity got modified by the learning process, as the animal, in base of its comprehension of the task rule, dynamically adapted the value assigned to stimulus.\\Reinforcement learning models capture such dynamic, parameterizing the learning functions. We proposed a learning-forgetting model, and we focused on the crucial terms for RPE signals, namely the uncertainty to get the reward and the prediction error. The prediction error, called $\delta$ in the model, was nothing but the mathematical difference between the expected reward value and the actual reward; the uncertainty, $\alpha$, was a time-dependent component, modulated by recent predictions and the prediction error $\delta$. This component mimicked the uncertainty of the animal to get the reward.\\This quantity was high at the beginning of the task and decreased ad the animal learnt the rule, to raise up again at the beginning of the reversal phase.%The prediction error was monotonically increasing with the difference between the reward and the expected reward value, it was positive when a not expected reward occurred and negative when vice-versa an expected reward did not occur. 
    Vice-versa, as we shown in \hyperref[chap:Overview]{~Chapter \ref*{chap:Overview}}, RPE signals in dopamine neurons are such that neuronal activity increases monotonically after the stimulus onset with the probability to get the reward, as consequence of the increase of the certainty of the animal to get the reward when it learnt the task. Whereas, at the reward delivery time, the neural activity is maximized when the surprise to get the reward is high, which means no prediction. In other words, across trials the peak of neural activity shifts back from the reward retrieval time to the stimulus onset as soon as the animal becomes able to predict the reward.\\If an assembly-pair conveyed RPE signals, the activity anti-correlated with the uncertainty in the stimulus window and correlated with the prediction error in the reward window.\\This was assessed with two Poisson linear regression of the assembly-pairs activity on the uncertainty ($\alpha$) in the CS window and on the prediction error ($\delta$) in the US window. The regression coefficients distributions confirmed that SPN-DAN assembly-pairs specifically conveyed RPE signals, this signals were not found in FSN-DAN assembly-pairs could instead represent motivational salience or hedonic signals.
\end{itemize}
  These findings provide a network coding mechanism of how dopaminergic learning signals promote value assignment to generate an assembly prediction code to dopaminergic midbrain neurons. 